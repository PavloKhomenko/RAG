{"question": "What is CLIP?", "contexts": ["while addressing real-world challenges. Share", "fine control to that technique. Share", "Input: text, images (up to 10 MB per file), audio (up to 10 MB and 3 minutes per file), video (up to 150 MB and 40 seconds per file) for a total of up to 32,768 tokens. Output: text, speech Performance: State of the art in some audio- and image-to-text benchmarks Training data: 18 trillion tokens of text (identical to Qwen2.5), 800 billion tokens of images and videos, 300 billion tokens of audio, 100 billion tokens of video with audio Undisclosed: Knowledge cutoff, output size, adapter architecture Availability: Weights free to download under the Apache 2.0 license. API price: Input: 0.4 Yuan per million tokens of text, 25 Yuan per million tokens of audio, 1.5 Yuan per million tokens of images/video. Output: 1.6 Yuan per million tokens of text with text-only input; 4.5 Yuan per million tokens of text with audio, video, or image input; 50 Yuan per million tokens of audio with any input. How it works: Qwen2.5-Omni 7B comprises a pretrained text transformer ( Qwen 2.5 7B ), pretrained", "The Batch Weekly Issues issue 287 Published Feb 5, 2025 Reading time 14 min read Published Feb 05, 2025 Reading time 14 min read Share Dear friends, A "10x engineer" — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don't seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more "10x professionals." There aren't already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they're not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely", "The Batch Weekly Issues issue 286 Published Jan 29, 2025 Reading time 13 min read Published Jan 29, 2025 Reading time 13 min read Share Dear friends, The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn't the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs. About a week ago, DeepSeek, a company based in China, released DeepSeek-R1 , a remarkable model whose performance on benchmarks is comparable to OpenAI's o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business"], "answer": "CLIP (Contrastive Language–Image Pretraining) is a neural network model developed by OpenAI that can understand and generate relationships between text and images. It is designed to learn visual concepts from natural language supervision, allowing it to perform tasks such as image classification, object detection, and zero-shot learning without needing task-specific training data. CLIP leverages a large dataset of images paired with text descriptions to learn a shared representation for both modalities, enabling it to effectively match images and descriptions, even for concepts it has not explicitly been trained on."}
{"question": "How does Qdrant store data?", "contexts": ["show. The User Is Always Right a Genius!!! OpenAI's most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results. What's new: OpenAI quickly withdrew an update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn't call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, it explained the source of the problem and promised to change its training methods to avoid overly agreeable output. Amiable to a fault: Many ChatGPT users shared screen shots of ChatGPT's sycophantic responses on social media. One user entered a make-believe query about choosing to save a toaster from a runaway trolley by diverting the vehicle and causing the death of three cows and two cats. "That's not 'wrong,'" ChatGPT responded. "You prioritized what mattered most to ." When a user informed ChatGPT that a vase he and", "produces research reports by scouring the web and reasoning over what it finds. What's new: OpenAI's deep research responds to users' requests by generating a detailed report based on hundreds of online sources. The system generates text output, with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro, but the company plans to roll it out to users of ChatGPT Plus, Team, and Enterprise. How it works: Deep research is an agent that uses OpenAI's o3 model, which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools, similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system. The system responds best to detailed prompts that specify the desired output (such as the desired information, comparisons, and format), the team said in its announcement video (which features Mark Chen, Josh Tobin,", "while addressing real-world challenges. Share", "generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning. OpenAI's GPT-4.5 Goes Big OpenAI launched GPT-4.5, which may be its last non-reasoning model. What's new: GPT-4.5 is available as a research preview. Unlike OpenAI's recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it's a huge model that was trained using a huge amount of computation. As OpenAI's biggest model to date, GPT-4.5 is very expensive to run, and the company is evaluating whether to offer it via API in the long term. Input/output: text and images in, text out. Voice and video interactions may be available in future updates. Availability/price: Via ChatGPT (currently ChatGPT Pro; soon ChatGPT Plus, Team, Enterprise, and Edu) and various APIs (Chat Completions, Assistants, and Batch). $75/$150 per million input/output tokens.", "out.) OpenAI limits the number API calls users can make per minute and per day depending on how frequently they use the API and how much money they've spent. Rate limits range from 5,000/4 million requests/tokens per per minute (Tier 3) to 30,000/150 million requests/tokens per minute (Tier 5), with higher limits for batch requests. o3-mini's system card highlights safety measures taken during the model's training. OpenAI notes that o3-mini's improved coding ability puts it at a medium risk for autonomous misuse, the first OpenAI model to be so flagged. What they're saying: Users praised o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to "chunkier" prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts. Behind the news: Days after releasing o3-mini, OpenAI launched deep research , a ChatGPT research agent based on o3. OpenAI had announced the o3 model family in"], "answer": "Qdrant stores data using a vector database approach. This involves storing data in the form of high-dimensional vectors, which are typically generated from unstructured data like text, images, or audio. These vectors are then indexed to enable efficient similarity search and retrieval. Qdrant is designed to handle large-scale vector data and provides features such as filtering, payload support, and integrations with machine learning frameworks to facilitate advanced search capabilities and machine learning workflows."}
{"question": "What is Retrieval-Augmented Generation?", "contexts": ["be. I think generating a pre-response followed by a full response, to quickly acknowledge the user's query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system's latency to around 0.5-1 seconds. Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize! Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you'll start building prototypes and see how far you can get! I'll keep building voice applications and sharing best", "while addressing real-world challenges. Share", "generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning. OpenAI's GPT-4.5 Goes Big OpenAI launched GPT-4.5, which may be its last non-reasoning model. What's new: GPT-4.5 is available as a research preview. Unlike OpenAI's recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it's a huge model that was trained using a huge amount of computation. As OpenAI's biggest model to date, GPT-4.5 is very expensive to run, and the company is evaluating whether to offer it via API in the long term. Input/output: text and images in, text out. Voice and video interactions may be available in future updates. Availability/price: Via ChatGPT (currently ChatGPT Pro; soon ChatGPT Plus, Team, Enterprise, and Edu) and various APIs (Chat Completions, Assistants, and Batch). $75/$150 per million input/output tokens.", "The Batch Machine Learning Research Article DeepSeek Sharpens Its Reasoning DeepSeek-R1, an affordable rival to OpenAI's o1 Machine Learning Research Business Large Language Models (LLMs) Published Jan 22, 2025 Reading time 4 min read Share A new open model rivals OpenAI's o1, and it's free to use or modify. What's new: DeepSeek released DeepSeek-R1 , a large language model that executes long lines of reasoning before producing output. The code and weights are licensed freely for commercial and personal use, including training new models on R1 outputs. The paper provides an up-close look at the training of a high-performance model that implements a chain of thought without explicit prompting. ( DeepSeek-R1-lite-preview came out in November with fewer parameters and a different base model.) Mixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by", "approaches to produce more generally useful embeddings. We're thinking: It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations. Share"], "answer": "Retrieval-Augmented Generation (RAG) is a technique that combines the strengths of information retrieval and text generation to enhance the performance of language models. In RAG, a language model is augmented with a retrieval mechanism that allows it to access a large database or corpus of text. When a query is presented to the model, the retrieval component first searches the database for relevant documents or information related to the query. This retrieved information is then used to inform and guide the text generation process, enabling the model to produce more accurate, relevant, and contextually grounded responses.\n\nRAG approaches are particularly useful in scenarios where the language model alone might not have sufficient information to generate a high-quality response, such as when dealing with highly specific or niche topics. By leveraging external knowledge sources through retrieval, RAG systems can provide more comprehensive and informative answers, making them suitable for applications like question answering, customer support, and conversational agents."}
{"question": "What are diffusion models?", "contexts": ["Mar 26, 2025 Faster Learning for Diffusion Models: Pretrained embeddings accelerate diffusion transformers' learning Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2. Mar 26, 2025", "while addressing real-world challenges. Share", "approaches to produce more generally useful embeddings. We're thinking: It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations. Share", "Alignment (REPA), a loss term for transformer-based diffusion. Key insight: Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn't need to learn how to embed an image from scratch. How it works: The authors modified DiT-XL/2 and SiT-XL/2 transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by", "team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task. At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better. Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to"], "answer": "Diffusion models are a type of generative model used in machine learning, particularly for image synthesis. They work by learning to remove noise from images that have been artificially corrupted with noise. The process involves two main steps: first, embedding the noisy image, and second, estimating the noise from this embedding. During inference, diffusion models start with pure noise and iteratively refine it to generate a new image. \n\nRecent advancements have shown that diffusion models can learn more efficiently by using pretrained embeddings. By incorporating a loss term that encourages the diffusion model to produce embeddings similar to those from a pretrained model, the learning process can be accelerated. This approach allows the model to focus more on estimating the noise rather than learning how to embed an image from scratch. This method has been applied to transformer-based latent diffusion models, such as the modified DiT-XL/2 and SiT-XL/2, which subtract noise from embeddings instead of directly from images. These models have shown success in generating high-quality images, similar to those found in datasets like ImageNet."}
{"question": "What is the role of transformers in neural networks?", "contexts": ["The Batch Machine Learning Research Article DeepSeek Sharpens Its Reasoning DeepSeek-R1, an affordable rival to OpenAI's o1 Machine Learning Research Business Large Language Models (LLMs) Published Jan 22, 2025 Reading time 4 min read Share A new open model rivals OpenAI's o1, and it's free to use or modify. What's new: DeepSeek released DeepSeek-R1 , a large language model that executes long lines of reasoning before producing output. The code and weights are licensed freely for commercial and personal use, including training new models on R1 outputs. The paper provides an up-close look at the training of a high-performance model that implements a chain of thought without explicit prompting. ( DeepSeek-R1-lite-preview came out in November with fewer parameters and a different base model.) Mixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by", "deliberate than previously understood. What's new: Emmanuel Ameisen and colleagues at Anthropic devised a method to study how transformers generate responses to specific prompts. They also studied Claude 3.5 Haiku's responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations. Key insight: A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable "features," or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model's output. How it works: The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features. The authors trained one cross-layer transcoder for each fully", "Mar 26, 2025 Faster Learning for Diffusion Models: Pretrained embeddings accelerate diffusion transformers' learning Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2. Mar 26, 2025", "applications, but to learn enough coding to use AI-assisted coding tools effectively. One question I'm asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that. When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn't know this language, and my paltry attempts at prompting could not deliver as effective a result. Similarly, scientists, analysts, marketers, recruiters, and", "team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task. At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better. Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to"], "answer": "Transformers play a critical role in modern neural networks, especially in the context of large language models and other applications involving sequential data. Their primary role is to process sequences of data (such as text) in a way that captures both local and global dependencies through mechanisms like attention. This allows transformers to effectively handle tasks that require understanding context over long sequences, such as language translation, text generation, and more complex reasoning tasks.\n\nThe key components of a transformer include:\n\n1. **Self-Attention Mechanism**: This allows the model to weigh the importance of different words in a sequence relative to each other, which helps in understanding context and relationships within the sequence.\n\n2. **Positional Encoding**: Since transformers do not inherently understand the order of sequences, positional encodings are added to input embeddings to give the model a sense of the position of words or tokens.\n\n3. **Layered Architecture**: Transformers use a stack of encoder and/or decoder layers, each comprising multiple attention heads and feedforward neural networks, to build deep representations of the input data.\n\nThe role of transformers extends beyond natural language processing. They are also used in areas like computer vision, where they learn complex patterns in image data, and in diffusion models, where pretrained embeddings can accelerate learning processes.\n\nOverall, transformers have revolutionized the field of AI by enabling models like large language models (LLMs) to perform tasks that require complex reasoning and understanding of context, as evidenced by models like DeepSeek-R1 and Claude 3.5 Haiku. These models effectively utilize transformer architectures to enhance their reasoning capabilities and interpretability."}
{"question": "How is multimodal learning useful?", "contexts": ["while addressing real-world challenges. Share", "evidence for longer-term effects. Why it matters: LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM's general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination. We're thinking: Although it relies on sophisticated technology, the authors' approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines. Faster Learning for Diffusion Models Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2. What's new: Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposed Representation", "AI Careers LLM Support for Tutors: GPT-4 boosts remote tutors' performance in real time, study finds Students benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors' effectiveness in real time. Mar 26, 2025", "applications, but to learn enough coding to use AI-assisted coding tools effectively. One question I'm asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that. When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn't know this language, and my paltry attempts at prompting could not deliver as effective a result. Similarly, scientists, analysts, marketers, recruiters, and", "team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task. At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better. Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to"], "answer": "Multimodal learning is useful for several reasons, particularly in the context of leveraging different types of data and enhancing educational and technological processes:\n\n1. **Enhanced Learning Experience**: By integrating multiple modes of information (such as text, images, and audio), learners can engage with content in various ways, which can accommodate different learning styles and preferences. This can lead to a more comprehensive understanding of the material.\n\n2. **Real-World Application**: Multimodal learning can address real-world challenges by combining insights from different data types. For instance, in education, large language models (LLMs) can draw on textual information and pedagogical insights to assist both students and teachers, enhancing the teaching process.\n\n3. **Improved Performance in AI Models**: In AI, multimodal approaches can improve model performance. For example, diffusion models can learn faster when they incorporate embeddings from pretrained models that might include visual and textual data, thus enriching the representation and learning process.\n\n4. **Versatility Across Disciplines**: The integration of different modalities makes teaching and learning applicable across various disciplines. For instance, using LLMs to apply teaching principles can extend beyond specific subjects like elementary math to a wide range of educational fields.\n\n5. **Facilitating Fine-Tuning and Optimization**: Multimodal learning can assist in fine-tuning AI models for specific tasks by providing a richer set of information, leading to better optimization without necessarily relying on larger models.\n\nIn summary, multimodal learning enhances educational outcomes, improves AI model performance, and provides flexible, real-world applications across multiple fields."}